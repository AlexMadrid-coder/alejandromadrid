<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[0, 6], [2, 5], [3, 9], [6, 7], [6, 8]], "layout": [[0.1111111111111111, 0.0], [0.3333333333333333, 0.0], [0.5555555555555556, 0.0], [0.7777777777777778, 0.0], [1.0, 0.0], [0.5555555555555556, 0.5], [0.1111111111111111, 0.5], [0.0, 1.0], [0.2222222222222222, 1.0], [0.7777777777777778, 0.5]], "plan": ["The proposed solution will involve using a simple machine learning model,\nspecifically a Random Forest Classifier, to predict whether passengers on the\nTitanic survived or not. This choice is motivated by the classifier's robustness\nand ability to handle both numerical and categorical data effectively. The model\nwill be trained using features such as 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch',\n'Fare', and 'Embarked', which are available in the training dataset. Missing\nvalues in 'Age' and 'Embarked' will be imputed using median and mode\nrespectively. The model's performance will be evaluated using accuracy, as it is\na straightforward metric suitable for binary classification tasks. Predictions\nwill be made on the test set and results will be saved in a 'submission.csv'\nfile in the './working' directory.", "The solution involves using a Logistic Regression model to predict the survival\nof passengers on the Titanic. Logistic Regression is chosen for its\neffectiveness in binary classification problems. The model will be trained using\nfeatures such as 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', and\n'Embarked'. Missing values in the 'Age' and 'Embarked' columns will be imputed\nusing median and mode respectively. The model's performance will be evaluated\nusing accuracy, which is appropriate for binary classification tasks.\nPredictions will be made on the test set and the results will be stored in a\n'submission.csv' file in the './working' directory.", "I will use a Decision Tree Classifier to predict the survival of passengers on\nthe Titanic. This model is simple yet effective for handling both numerical and\ncategorical data, and it provides good interpretability. The features used will\ninclude 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', and 'Embarked'.\nMissing values in 'Age' will be filled with the median age, and 'Embarked' will\nbe filled with the mode of the column. The model's performance will be evaluated\nusing accuracy on a validation set. Predictions will be made on the test dataset\nand saved in a 'submission.csv' file in the './working' directory.", "For this solution, a Decision Tree Classifier will be employed to predict\nsurvival on the Titanic. This model is selected due to its interpretability and\nease of implementation. It will be trained using features such as 'Pclass',\n'Sex', 'Age', 'SibSp', 'Parch', 'Fare', and 'Embarked'. The 'Age' and 'Embarked'\ncolumns will have missing values imputed using median and mode respectively. The\nmodel's performance will be evaluated using accuracy, which is suitable for\nbinary classification tasks. Predictions will be made on the test set and saved\nin a 'submission.csv' file in the './working' directory.", "The proposed solution will utilize a Decision Tree Classifier to predict the\nsurvival of passengers on the Titanic. This classifier is chosen for its\ninterpretability and ability to handle both numerical and categorical data. Key\nfeatures such as 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', and\n'Embarked' will be used for training. Missing values in the 'Age' and 'Embarked'\ncolumns will be imputed using the median and mode respectively. The model's\nperformance will be evaluated using accuracy, which is suitable for binary\nclassification tasks. Predictions will be made on the test set and results will\nbe saved in a 'submission.csv' file in the './working' directory.", "The error in the previous implementation occurs due to missing values in the\n'Fare' feature of the test dataset, which wasn't explicitly handled in the\npreprocessing function. To fix this, we will update the preprocessing function\nto fill missing 'Fare' values with the median fare from the training data.\nAdditionally, we will ensure that the label encoders for categorical variables\n('Sex' and 'Embarked') are fitted only on the training data to avoid data\nleakage and then used to transform both the training and test datasets. This\napproach will maintain consistency in data handling and prevent any\ndiscrepancies due to different encoding between training and test data.", "To enhance the performance of the RandomForestClassifier, we can tune its\nhyperparameters using grid search. Hyperparameters such as 'n_estimators',\n'max_depth', 'min_samples_split', and 'min_samples_leaf' can significantly\naffect the model's accuracy. Implementing a grid search with cross-validation\nwill allow us to systematically explore different combinations of these\nparameters to find the best model configuration. This method can potentially\nimprove the model's generalization capability and increase the validation\naccuracy.", "To further enhance the RandomForest model's performance, I propose focusing on\nfeature engineering, particularly by creating new features that could be\npredictive of survival. One common strategy is to create a feature that combines\n'SibSp' (number of siblings/spouses aboard) and 'Parch' (number of\nparents/children aboard) into a single feature called 'FamilySize'. This new\nfeature represents the total number of family members on board, which might\nprovide more predictive power than 'SibSp' and 'Parch' separately. Additionally,\nintroducing interaction terms or polynomial features could unveil complex\nrelationships in the data that a tree-based model might exploit.", "To enhance the predictive power of our RandomForest model, I propose to engineer\na new feature called \"Title\" derived from the \"Name\" column of the dataset. The\ntitle (like Mr., Mrs., Miss) can be indicative of gender, marital status, and\nsocial status, which might correlate with survival rates. We'll extract the\ntitle from the \"Name\" field, encode it, and include it in our model training.\nThis feature engineering step could reveal additional patterns that simple\ndemographic or ticket-related features might miss, potentially improving the\nmodel's accuracy.", "The bug in the provided code is due to a typo in the function name\n`train_test_test_split` which should be corrected to `train_test_split`. This\nfunction is used to split the training data into a training subset and a\nvalidation subset to evaluate the model's performance. After correcting this\ntypo, the code will correctly split the data, fit the model on the training\nsubset, and evaluate it on the validation subset. Additionally, the output file\npath should be corrected to match the specified directory\n`./Data/submission.csv` instead of `./working/submission.csv`."], "code": ["import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n# Prepare the data by selecting features and dealing with missing values\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nX = train_data[features]\ny = train_data[\"Survived\"]\nX_test = test_data[features]\n\n# Impute missing values\nimputer = SimpleImputer(strategy=\"median\")\nX[\"Age\"] = imputer.fit_transform(X[[\"Age\"]])\nX_test[\"Age\"] = imputer.transform(X_test[[\"Age\"]])\n\nimputer_fare = SimpleImputer(strategy=\"median\")\nX[\"Fare\"] = imputer_fare.fit_transform(X[[\"Fare\"]])\nX_test[\"Fare\"] = imputer_fare.transform(X_test[[\"Fare\"]])\n\n# Impute Embarked with the mode\nmode_embarked = train_data[\"Embarked\"].mode()[0]\nX[\"Embarked\"].fillna(mode_embarked, inplace=True)\nX_test[\"Embarked\"].fillna(mode_embarked, inplace=True)\n\n# Encode categorical data\nencoder = LabelEncoder()\nX[\"Sex\"] = encoder.fit_transform(X[\"Sex\"])\nX_test[\"Sex\"] = encoder.transform(X_test[\"Sex\"])\n\nX[\"Embarked\"] = encoder.fit_transform(X[\"Embarked\"])\nX_test[\"Embarked\"] = encoder.transform(X_test[\"Embarked\"])\n\n# Splitting train data for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model training\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Validate the model\ny_pred = model.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(f\"Validation Accuracy: {accuracy}\")\n\n# Predict on test set\npredictions = model.predict(X_test)\n\n# Create submission file\noutput = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": predictions})\noutput.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n\n# Preprocessing\ndef preprocess_data(data):\n    # Fill missing values\n    data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n    data[\"Fare\"].fillna(data[\"Fare\"].median(), inplace=True)\n\n    # Encode categorical data\n    label_encoder = LabelEncoder()\n    data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n    data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n\n    # Select features\n    features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n    X = data[features]\n    if \"Survived\" in data.columns:\n        y = data[\"Survived\"]\n        return X, y\n    return X\n\n\nX, y = preprocess_data(train_data)\nX_test = preprocess_data(test_data)\n\n# Splitting the dataset for training and validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\n# Model training\nmodel = LogisticRegression(max_iter=400)\nmodel.fit(X_train_scaled, y_train)\n\n# Validation\ny_pred = model.predict(X_val_scaled)\naccuracy = accuracy_score(y_val, y_pred)\nprint(f\"Validation Accuracy: {accuracy}\")\n\n# Predict on test data\ntest_predictions = model.predict(X_test_scaled)\n\n# Create submission file\nsubmission = pd.DataFrame(\n    {\"PassengerId\": test_data[\"PassengerId\"], \"Survived\": test_predictions}\n)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n\n# Preprocess data\ndef preprocess_data(data):\n    # Fill missing 'Age' with median and 'Embarked' with mode\n    data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n\n    # Encode categorical variables\n    label_encoder = LabelEncoder()\n    data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n    data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n\n    return data\n\n\ntrain_data = preprocess_data(train_data)\ntest_data = preprocess_data(test_data)\n\n# Select features\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nX_train = train_data[features]\ny_train = train_data[\"Survived\"]\nX_test = test_data[features]\n\n# Split train data for validation\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train_split, y_train_split)\n\n# Validate model\npredictions_val = model.predict(X_val_split)\naccuracy = accuracy_score(y_val_split, predictions_val)\nprint(\"Validation Accuracy:\", accuracy)\n\n# Predict on test data\npredictions_test = model.predict(X_test)\n\n# Save predictions to submission.csv\noutput = pd.DataFrame(\n    {\"PassengerId\": test_data.PassengerId, \"Survived\": predictions_test}\n)\noutput.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n\n# Preprocess data\ndef preprocess_data(data):\n    # Impute missing values\n    data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n    data[\"Fare\"].fillna(data[\"Fare\"].median(), inplace=True)\n\n    # Convert categorical variables to numeric\n    le = LabelEncoder()\n    data[\"Sex\"] = le.fit_transform(data[\"Sex\"])\n    data[\"Embarked\"] = le.fit_transform(data[\"Embarked\"])\n\n    # Select features\n    features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n    X = data[features]\n    return X\n\n\nX_train = preprocess_data(train_data)\ny_train = train_data[\"Survived\"]\nX_test = preprocess_data(test_data)\n\n# Train model\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# Predict on test set\npredictions = model.predict(X_test)\n\n# Save output\noutput = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": predictions})\noutput.to_csv(\"./working/submission.csv\", index=False)\n\n# Evaluate model on a hold-out validation set\nX_train_dev, X_val, y_train_dev, y_val = train_test_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\nmodel.fit(X_train_dev, y_train_dev)\nval_predictions = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_predictions)\nprint(\"Validation Accuracy:\", val_accuracy)\n", "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n\n# Prepare data\ndef preprocess_data(data):\n    # Handle missing values\n    imputer = SimpleImputer(strategy=\"median\")\n    data[\"Age\"] = imputer.fit_transform(data[[\"Age\"]])\n\n    # Impute Embarked with mode\n    mode_embarked = data[\"Embarked\"].mode()[0]\n    data[\"Embarked\"].fillna(mode_embarked, inplace=True)\n\n    # Label encoding for categorical variables\n    label_encoder = LabelEncoder()\n    data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n    data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n\n    return data\n\n\ntrain_data = preprocess_data(train_data)\ntest_data = preprocess_data(test_data)\n\n# Select features\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nX_train = train_data[features]\ny_train = train_data[\"Survived\"]\nX_test = test_data[features]\n\n# Split training data for validation\nX_train_part, X_val_part, y_train_part, y_val_part = train_test_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\n# Train Decision Tree model\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train_part, y_train_part)\n\n# Validate the model\npredictions_val = model.predict(X_val_part)\naccuracy = accuracy_score(y_val_part, predictions_val)\nprint(\"Validation Accuracy:\", accuracy)\n\n# Predict on test data\npredictions_test = model.predict(X_test)\n\n# Create submission file\nsubmission = pd.DataFrame(\n    {\"PassengerId\": test_data[\"PassengerId\"], \"Survived\": predictions_test}\n)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n\n# Preprocess data\ndef preprocess_data(train, test):\n    # Handle missing values\n    for column in [\"Age\", \"Fare\"]:\n        train[column].fillna(train[column].median(), inplace=True)\n        test[column].fillna(train[column].median(), inplace=True)\n    train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True)\n    test[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True)\n\n    # Encode categorical variables\n    label_encoder_sex = LabelEncoder()\n    label_encoder_embarked = LabelEncoder()\n\n    train[\"Sex\"] = label_encoder_sex.fit_transform(train[\"Sex\"])\n    test[\"Sex\"] = label_encoder_sex.transform(test[\"Sex\"])\n\n    train[\"Embarked\"] = label_encoder_embarked.fit_transform(train[\"Embarked\"])\n    test[\"Embarked\"] = label_encoder_embarked.transform(test[\"Embarked\"])\n\n    return train, test\n\n\ntrain_data, test_data = preprocess_data(train_data, test_data)\n\n# Select features\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nX_train = train_data[features]\ny_train = train_data[\"Survived\"]\nX_test = test_data[features]\n\n# Split train data for validation\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train_split, y_train_split)\n\n# Validate model\npredictions_val = model.predict(X_val_split)\naccuracy = accuracy_score(y_val_split, predictions_val)\nprint(\"Validation Accuracy:\", accuracy)\n\n# Predict on test data\npredictions_test = model.predict(X_test)\n\n# Save predictions to submission.csv\noutput = pd.DataFrame(\n    {\"PassengerId\": test_data.PassengerId, \"Survived\": predictions_test}\n)\noutput.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n# Prepare the data by selecting features and dealing with missing values\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nX = train_data[features]\ny = train_data[\"Survived\"]\nX_test = test_data[features]\n\n# Impute missing values\nimputer = SimpleImputer(strategy=\"median\")\nX[\"Age\"] = imputer.fit_transform(X[[\"Age\"]])\nX_test[\"Age\"] = imputer.transform(X_test[[\"Age\"]])\n\nimputer_fare = SimpleImputer(strategy=\"median\")\nX[\"Fare\"] = imputer_fare.fit_transform(X[[\"Fare\"]])\nX_test[\"Fare\"] = imputer_fare.transform(X_test[[\"Fare\"]])\n\n# Impute Embarked with the mode\nmode_embarked = train_data[\"Embarked\"].mode()[0]\nX[\"Embarked\"].fillna(mode_embarked, inplace=True)\nX_test[\"Embarked\"].fillna(mode_embarked, inplace=True)\n\n# Encode categorical data\nencoder = LabelEncoder()\nX[\"Sex\"] = encoder.fit_transform(X[\"Sex\"])\nX_test[\"Sex\"] = encoder.transform(X_test[\"Sex\"])\n\nX[\"Embarked\"] = encoder.fit_transform(X[\"Embarked\"])\nX_test[\"Embarked\"] = encoder.transform(X_test[\"Embarked\"])\n\n# Splitting train data for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model training with Grid Search\nparam_grid = {\n    \"n_estimators\": [100, 200, 300],\n    \"max_depth\": [None, 10, 20, 30],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n}\n\nmodel = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2\n)\ngrid_search.fit(X_train, y_train)\n\n# Validate the model\ny_pred = grid_search.best_estimator_.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(f\"Validation Accuracy: {accuracy}\")\n\n# Predict on test set\npredictions = grid_search.best_estimator_.predict(X_test)\n\n# Create submission file\noutput = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": predictions})\noutput.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n# Prepare the data by selecting features and dealing with missing values\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nX = train_data[features]\ny = train_data[\"Survived\"]\nX_test = test_data[features]\n\n# Feature Engineering: Create new feature 'FamilySize'\nX[\"FamilySize\"] = X[\"SibSp\"] + X[\"Parch\"]\nX_test[\"FamilySize\"] = X_test[\"SibSp\"] + X_test[\"Parch\"]\n\n# Impute missing values\nimputer = SimpleImputer(strategy=\"median\")\nX[\"Age\"] = imputer.fit_transform(X[[\"Age\"]])\nX_test[\"Age\"] = imputer.transform(X_test[[\"Age\"]])\n\nimputer_fare = SimpleImputer(strategy=\"median\")\nX[\"Fare\"] = imputer_fare.fit_transform(X[[\"Fare\"]])\nX_test[\"Fare\"] = imputer_fare.transform(X_test[[\"Fare\"]])\n\n# Impute Embarked with the mode\nmode_embarked = train_data[\"Embarked\"].mode()[0]\nX[\"Embarked\"].fillna(mode_embarked, inplace=True)\nX_test[\"Embarked\"].fillna(mode_embarked, inplace=True)\n\n# Encode categorical data\nencoder = LabelEncoder()\nX[\"Sex\"] = encoder.fit_transform(X[\"Sex\"])\nX_test[\"Sex\"] = encoder.transform(X_test[\"Sex\"])\n\nX[\"Embarked\"] = encoder.fit_transform(X[\"Embarked\"])\nX_test[\"Embarked\"] = encoder.transform(X_test[\"Embarked\"])\n\n# Splitting train data for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model training with Grid Search\nparam_grid = {\n    \"n_estimators\": [100, 200, 300],\n    \"max_depth\": [None, 10, 20, 30],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n}\n\nmodel = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2\n)\ngrid_search.fit(X_train, y_train)\n\n# Validate the model\ny_pred = grid_search.best_estimator_.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(f\"Validation Accuracy: {accuracy}\")\n\n# Predict on test set\npredictions = grid_search.best_estimator_.predict(X_test)\n\n# Create submission file\noutput = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": predictions})\noutput.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n# Extract titles from names\ntrain_data[\"Title\"] = train_data[\"Name\"].apply(\n    lambda x: x.split(\",\")[1].split(\".\")[0].strip()\n)\ntest_data[\"Title\"] = test_data[\"Name\"].apply(\n    lambda x: x.split(\",\")[1].split(\".\")[0].strip()\n)\n\n# Prepare the data by selecting features and dealing with missing values\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Title\"]\nX = train_data[features]\ny = train_data[\"Survived\"]\nX_test = test_data[features]\n\n# Impute missing values\nimputer = SimpleImputer(strategy=\"median\")\nX[\"Age\"] = imputer.fit_transform(X[[\"Age\"]])\nX_test[\"Age\"] = imputer.transform(X_test[[\"Age\"]])\n\nimputer_fare = SimpleImputer(strategy=\"median\")\nX[\"Fare\"] = imputer_fare.fit_transform(X[[\"Fare\"]])\nX_test[\"Fare\"] = imputer_fare.transform(X_test[[\"Fare\"]])\n\n# Impute Embarked with the mode\nmode_embarked = train_data[\"Embarked\"].mode()[0]\nX[\"Embarked\"].fillna(mode_embarked, inplace=True)\nX_test[\"Embarked\"].fillna(mode_embarked, inplace=True)\n\n# Encode categorical data\nencoder = LabelEncoder()\nX[\"Sex\"] = encoder.fit_transform(X[\"Sex\"])\nX_test[\"Sex\"] = encoder.transform(X_test[\"Sex\"])\n\nX[\"Embarked\"] = encoder.fit_transform(X[\"Embarked\"])\nX_test[\"Embarked\"] = encoder.transform(X_test[\"Embarked\"])\n\nX[\"Title\"] = encoder.fit_transform(X[\"Title\"])\nX_test[\"Title\"] = encoder.transform(X_test[\"Title\"])\n\n# Splitting train data for validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model training with Grid Search\nparam_grid = {\n    \"n_estimators\": [100, 200, 300],\n    \"max_depth\": [None, 10, 20, 30],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n}\n\nmodel = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2\n)\ngrid_search.fit(X_train, y_train)\n\n# Validate the model\ny_pred = grid_search.best_estimator_.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(f\"Validation Accuracy: {accuracy}\")\n\n# Predict on test set\npredictions = grid_search.best_estimator_.predict(X_test)\n\n# Create submission file\noutput = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": predictions})\noutput.to_csv(\"./working/submission.csv\", index=False)\n", "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data\ntrain_data = pd.read_csv(\"./input/train.csv\")\ntest_data = pd.read_csv(\"./input/test.csv\")\n\n\n# Preprocess data\ndef preprocess_data(data):\n    # Impute missing values\n    data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n    data[\"Fare\"].fillna(data[\"Fare\"].median(), inplace=True)\n\n    # Convert categorical variables to numeric\n    le = LabelEncoder()\n    data[\"Sex\"] = le.fit_transform(data[\"Sex\"])\n    data[\"Embarked\"] = le.fit_transform(data[\"Embarked\"])\n\n    # Select features\n    features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n    X = data[features]\n    return X\n\n\nX_train = preprocess_data(train_data)\ny_train = train_data[\"Survived\"]\nX_test = preprocess_data(test_data)\n\n# Train model\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# Predict on test set\npredictions = model.predict(X_test)\n\n# Save output\noutput = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": predictions})\noutput.to_csv(\"./Data/submission.csv\", index=False)\n\n# Evaluate model on a hold-out validation set\nX_train_dev, X_val, y_train_dev, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\nmodel.fit(X_train_dev, y_train_dev)\nval_predictions = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_predictions)\nprint(\"Validation Accuracy:\", val_accuracy)\n"], "term_out": ["Validation Accuracy: 0.8212290502793296\nExecution time: 3 seconds seconds (time limit is an hour).", "Validation Accuracy: 0.8044692737430168\nExecution time: a second seconds (time limit is an hour).", "Validation Accuracy: 0.7821229050279329\nTraceback (most recent call last):\n  File \"runfile.py\", line 51, in <module>\n    predictions_test = model.predict(X_test)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 426, in predict\n    X = self._validate_X_predict(X, check_input)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 392, in _validate_X_predict\n    X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/base.py\", line 565, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_params)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\nExecution time: a second seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 47, in <module>\n    X_train_dev, X_val, y_train_dev, y_val = train_test_test_split(\nNameError: name 'train_test_test_split' is not defined. Did you mean: 'train_test_split'?\nExecution time: a second seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 41, in <module>\n    X_train_part, X_val_part, y_train_part, y_val_part = train_test_test_split(\nNameError: name 'train_test_test_split' is not defined. Did you mean: 'train_test_split'?\nExecution time: a second seconds (time limit is an hour).", "Validation Accuracy: 0.7821229050279329\nExecution time: a second seconds (time limit is an hour).", "Fitting 3 folds for each of 108 candidates, totalling 324 fits\nValidation Accuracy: 0.8491620111731844\nExecution time: 43 seconds seconds (time limit is an hour).", "Fitting 3 folds for each of 108 candidates, totalling 324 fits\nValidation Accuracy: 0.8212290502793296\nExecution time: 41 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/_encode.py\", line 224, in _encode\n    return _map_to_integer(values, uniques)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/_encode.py\", line 164, in _map_to_integer\n    return np.array([table[v] for v in values])\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/_encode.py\", line 164, in <listcomp>\n    return np.array([table[v] for v in values])\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/_encode.py\", line 158, in __missing__\n    raise KeyError(key)\nKeyError: 'Dona'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"runfile.py\", line 49, in <module>\n    X_test[\"Title\"] = encoder.transform(X_test[\"Title\"])\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py\", line 139, in transform\n    return _encode(y, uniques=self.classes_)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/sklearn/utils/_encode.py\", line 226, in _encode\n    raise ValueError(f\"y contains previously unseen labels: {str(e)}\")\nValueError: y contains previously unseen labels: 'Dona'\nExecution time: a second seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 43, in <module>\n    output.to_csv(\"./Data/submission.csv\", index=False)\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/pandas/core/generic.py\", line 3902, in to_csv\n    return DataFrameRenderer(formatter).to_csv(\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/pandas/io/formats/format.py\", line 1152, in to_csv\n    csv_formatter.save()\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/pandas/io/formats/csvs.py\", line 247, in save\n    with get_handle(\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/pandas/io/common.py\", line 739, in get_handle\n    check_parent_directory(str(handle))\n  File \"/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/pandas/io/common.py\", line 604, in check_parent_directory\n    raise OSError(rf\"Cannot save file into a non-existent directory: '{parent}'\")\nOSError: Cannot save file into a non-existent directory: 'Data'\nExecution time: a second seconds (time limit is an hour)."], "analysis": ["The code executed successfully and achieved a validation accuracy of 82.12% on the validation set. This indicates a reasonable performance of the RandomForestClassifier model on the given dataset, using the selected features and preprocessing steps.", "The script successfully processes the Titanic dataset, performs feature engineering, and fits a Logistic Regression model. The validation accuracy achieved is 80.45%, indicating a decent model performance for this binary classification task.", "The script encountered a bug due to NaN values in the test data when predicting survival. This error occurred because the 'Fare' feature in the test dataset contains NaN values, which were not handled before making predictions. A fix would involve imputing missing values for 'Fare' in the test dataset before using the model to make predictions.", "The code execution failed due to a typo in the function name `train_test_test_split`, which should be `train_test_split`. To fix the bug, replace `train_test_test_split` with `train_test_split` in the code.", "There is a typo in the function name. The correct function name should be 'train_test_split', not 'train_test_test_split'. To fix this bug, replace 'train_test_test_split' with 'train_test_split' on line 41.", "The model achieved a validation accuracy of approximately 78.21% on the split validation set. This indicates that the decision tree classifier was able to predict the survival status of the passengers with a reasonable level of accuracy based on the provided features.", "The script successfully executed a grid search for hyperparameter tuning on a RandomForestClassifier and achieved a validation accuracy of 84.92%. This indicates a high level of predictive performance on the validation set, which is a subset of the training data. The model was trained with various hyperparameters and the best model was used to predict survival on the Titanic dataset.", "The script successfully executed a grid search for hyperparameter tuning on a RandomForestClassifier to predict Titanic survival. It achieved a validation accuracy of 82.12%. This indicates a robust performance, considering the model complexity and feature engineering applied.", "The script encountered a KeyError due to an unseen label 'Dona' in the 'Title' feature when transforming the test set. This issue arises because the training and test datasets have different sets of titles, and the LabelEncoder was fitted only on the training set. To fix this, one should fit the LabelEncoder on the combined set of titles from both training and test datasets before transforming them separately. This ensures all possible categories are known to the encoder.", "The code execution failed due to an attempt to save the output CSV file in a non-existent directory named 'Data'. To fix this, ensure that the directory exists before saving the file, or create it programmatically using `os.makedirs('./Data', exist_ok=True)` before the `output.to_csv()` line."], "exp_name": "0-inquisitive-sparkling-kagu", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
