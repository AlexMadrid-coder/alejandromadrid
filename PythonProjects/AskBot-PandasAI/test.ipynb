{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Askbot using PandasAI library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation of the exercise is to parse the audio from the microphone to text, and and ask to OpenAI model what you said."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote every key i need to use the API's like pandasai or openai into a .env file in my root project directory, if u want to use the app you should make one or copy my method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PandasAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandasai\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai.llm.openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "# # Audio detection\n",
    "import pyaudio\n",
    "import wave\n",
    "import audioop\n",
    "import webrtcvad\n",
    "# Speech2Text and Text2Speech\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "# Keys\n",
    "keys = dotenv_values(\"../../.env-token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "THRESHOLD = 500\n",
    "SILENCE_TIMEOUT = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment key\n",
    "os.environ[\"PANDASAI_API_KEY\"] = keys[\"PANDASAI_API_KEY\"]\n",
    "llm = OpenAI(api_token=keys[\"OPENAI_API_KEY\"])\n",
    "# Pyaudio\n",
    "audio = pyaudio.PyAudio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will record audio till we stay 4 seconds without talk\n",
    "\n",
    "stream = audio.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=CHUNK\n",
    ")\n",
    "print(\"You can talk now.\\nRecording...\")\n",
    "#\n",
    "frames = []\n",
    "silence_counter = 0\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(3) # <-- Aggressive detection to record voice\n",
    "\n",
    "while True:\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "    is_speech = vad.is_speech(data, RATE)\n",
    "    if not is_speech:\n",
    "        silence_counter += 1\n",
    "    else:\n",
    "        silence_counter = 0\n",
    "    if silence_counter >= int( SILENCE_TIMEOUT * ( RATE / CHUNK )):\n",
    "        break\n",
    "\n",
    "print(\"\\nFinished recording.\")\n",
    "\n",
    "# Stop stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "\n",
    "# Stop PyAudio\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded audio as .wav file\n",
    "with wave.open(\"Audios/recorded_audio.wav\", \"wb\") as audio_recorded:\n",
    "    audio_recorded.setnchannels(CHANNELS)\n",
    "    audio_recorded.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    audio_recorded.setframerate(RATE)\n",
    "    audio_recorded.writeframes(b\"\".join(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we record and saved the audio we still have to process it with speech2text and ask the question to OpenAI.\n",
    "\n",
    "When we complete every step, the we would just have convert to speech the generated text and play it (text2speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech2Text + Text2Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = sr.Recognizer()\n",
    "# This function says everything that is in text variable\n",
    "def Text2Speech(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    \n",
    "def Speech2Text(audioFile):\n",
    "    with sr.AudioFile(audio) as audioFile:\n",
    "        audio = r.record(audioFile)\n",
    "        print(\"Audio loaded\")\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print(text)\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return e\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'audio' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# speech2text try\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mSpeech2Text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAudios/recorded_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mSpeech2Text\u001b[0;34m(audioFile)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mSpeech2Text\u001b[39m(audioFile):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mAudioFile(\u001b[43maudio\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m audioFile:\n\u001b[1;32m     10\u001b[0m         audio \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrecord(audioFile)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'audio' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# speech2text try\n",
    "text = Speech2Text(\"Audios/recorded_audio.wav\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# text2speech try\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m Text2Speech(\u001b[43mtext\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# text2speech try\n",
    "Text2Speech(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
