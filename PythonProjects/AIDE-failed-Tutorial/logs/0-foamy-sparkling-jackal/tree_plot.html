<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[1, 5], [4, 6]], "layout": [[0.0, 0.0], [0.25, 0.0], [0.5, 0.0], [0.75, 0.0], [1.0, 0.0], [0.25, 1.0], [1.0, 1.0]], "plan": ["For this task, I will focus on the dataset\n`diabetes_binary_5050split_health_indicators_BRFSS2015.csv` which is balanced\nbetween individuals with and without diabetes. I will use a simple logistic\nregression model to predict the binary diabetes outcome. Logistic regression is\nchosen for its interpretability and efficiency, especially given the relatively\nstraightforward nature of the dataset. I will preprocess the data by scaling\nnumerical features to ensure that the logistic regression model performs\noptimally. The model will be evaluated using accuracy and f1-score, which are\nsuitable metrics for binary classification tasks. I'll use a train-test split\nfor model validation.", "For this task, I will use the Random Forest classifier to predict diabetes\nstatus from the balanced dataset\n`diabetes_binary_5050split_health_indicators_BRFSS2015.csv`. Random Forest is\nrobust to overfitting and works well with categorical and numerical data, making\nit suitable for this dataset with mixed feature types. I will preprocess the\ndata by encoding categorical variables and scaling numerical variables. The\nmodel will be evaluated using cross-validation to ensure the reliability of the\nresults, focusing on metrics such as accuracy and F1-score to measure\nperformance.", "For this task, I will utilize a Random Forest Classifier to predict the binary\ndiabetes outcome from the\n`diabetes_binary_5050split_health_indicators_BRFSS2015.csv` dataset. Random\nForest is a robust and versatile model that can handle both linear and non-\nlinear relationships and is less likely to overfit compared to other algorithms.\nIt also handles imbalanced data well, which is suitable given the nature of our\ndataset. I will preprocess the data by encoding categorical variables and\nscaling numerical features. The model will be evaluated using accuracy,\nF1-score, and the root mean squared logarithmic error (RMSLE), which are\nsuitable metrics for binary classification tasks with balanced data. I will use\na train-test split for model validation.", "In this solution, I will use a Support Vector Machine (SVM) classifier to\npredict the binary diabetes outcome from the\n`diabetes_binary_5050split_health_indicators_BRFSS2015.csv` dataset. SVM is\neffective for binary classification and can handle both linear and non-linear\nboundaries depending on the kernel used. I will preprocess the data by scaling\nthe features using StandardScaler as SVM is sensitive to the magnitude of data.\nThe model will be evaluated using accuracy and F1-score, which are suitable for\nbinary classification. I will use a train-test split for model validation. The\ngoal is to provide a model that can accurately classify individuals as having\ndiabetes or not based on health indicators.", "I will use a Gradient Boosting Classifier from the scikit-learn library to\npredict the binary diabetes outcome using the\n`diabetes_binary_5050split_health_indicators_BRFSS2015.csv` dataset. Gradient\nBoosting is effective for handling both bias and variance in the data and is\nparticularly useful for imbalanced datasets like this one, even though this\nspecific split is balanced. The model will be trained using all available\nfeatures without any feature selection or engineering to keep the model simple.\nData will be preprocessed by encoding categorical variables and scaling\nnumerical features. The model will be evaluated using accuracy and F1-score,\nwhich are suitable metrics for binary classification tasks. The implementation\nwill use a train-test split for model validation.", "The bug in the previous implementation is due to a typo in the function name\n`train_test_test_split` which should be `train_test_split`. This will be\ncorrected to properly split the dataset into training and testing sets.\nAdditionally, the RMSLE metric will be included using the `make_scorer` function\nfrom `sklearn.metrics` since it is not directly available. The f1-score will be\ncalculated for binary classification, and the code will ensure it handles any\ncategorical features properly, although the dataset only contains numerical\nfeatures.", "The main issue in the previous implementation is a typo in the function name\n`train_test_test_split` which should be corrected to `train_test_split` to\nproperly split the dataset into training and testing sets. Additionally, the\ncode should handle the evaluation of the model using RMSLE alongside accuracy\nand F1 score. Since RMSLE is not directly available in sklearn, it will be\ncomputed using the log and mean_squared_error functions. The corrected code will\nload the data, preprocess it, split it, train a Gradient Boosting Classifier,\nand finally evaluate the model using the specified metrics."], "code": ["import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Load the data\ndata_path = \"./input/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\"\ndata = pd.read_csv(data_path)\n\n# Separate features and target\nX = data.drop(\"Diabetes_binary\", axis=1)\ny = data[\"Diabetes_binary\"]\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize and train the logistic regression model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test_scaled)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n", "import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Load data\ndata = pd.read_csv(\"./input/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n\n# Separate features and target\nX = data.drop([\"Diabetes_binary\"], axis=1)\ny = data[\"Diabetes_binary\"]\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define categorical and numerical features\ncategorical_features = X.columns[X.dtypes == \"object\"].tolist()\nnumerical_features = X.columns[X.dtypes != \"object\"].tolist()\n\n# Create a column transformer to handle preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numerical_features),\n        (\"cat\", \"passthrough\", categorical_features),\n    ]\n)\n\n# Create a pipeline with preprocessing and classifier\nclf = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\n\n# Train the model using cross-validation\nscores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\nf1_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"f1\")\n\n# Print the mean of the cross-validated scores\nprint(f\"Cross-validated Accuracy: {scores.mean():.4f}\")\nprint(f\"Cross-validated F1 Score: {f1_scores.mean():.4f}\")\n", "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv(\"./input/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n\n# Separate features and target\nX = data.drop([\"Diabetes_binary\"], axis=1)\ny = data[\"Diabetes_binary\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize and train the Random Forest Classifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test_scaled)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nrmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"RMSLE: {rmsle:.4f}\")\n", "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Load the dataset\ndata = pd.read_csv(\"./input/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n\n# Separate features and target\nX = data.drop(columns=[\"Diabetes_binary\"])\ny = data[\"Diabetes_binary\"]\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize and train the SVM classifier\nsvm_classifier = SVC(kernel=\"linear\", random_state=42)\nsvm_classifier.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = svm_classifier.predict(X_test_scaled)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n", "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.pipeline import make_pipeline\n\n# Load the data\ndata_path = \"./input/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\"\ndata = pd.read_csv(data_path)\n\n# Separate features and target\nX = data.drop(columns=[\"Diabetes_binary\"])\ny = data[\"Diabetes_binary\"]\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create a pipeline with scaling and classifier\npipeline = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=42))\n\n# Train the model\npipeline.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = pipeline.predict(X_test)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n", "import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    mean_squared_log_error,\n    make_scorer,\n)\nimport numpy as np\n\n# Load data\ndata = pd.read_csv(\"./input/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n\n# Separate features and target\nX = data.drop([\"Diabetes_binary\"], axis=1)\ny = data[\"Diabetes_binary\"]\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create a pipeline with preprocessing and classifier\nclf = Pipeline(\n    steps=[\n        (\"scaler\", StandardScaler()),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\n\n\n# Define RMSLE scorer\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\n\nrmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n\n# Train the model using cross-validation\nscores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\nf1_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"f1\")\nrmsle_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=rmsle_scorer)\n\n# Print the mean of the cross-validated scores\nprint(f\"Cross-validated Accuracy: {scores.mean():.4f}\")\nprint(f\"Cross-validated F1 Score: {f1_scores.mean():.4f}\")\nprint(f\"Cross-validated RMSLE: {rmsle_scores.mean():.4f}\")\n", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_error\nfrom sklearn.pipeline import make_pipeline\n\n# Load the data\ndata_path = \"./input/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\"\ndata = pd.read_csv(data_path)\n\n# Separate features and target\nX = data.drop(columns=[\"Diabetes_binary\"])\ny = data[\"Diabetes_binary\"]\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create a pipeline with scaling and classifier\npipeline = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=42))\n\n# Train the model\npipeline.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = pipeline.predict(X_test)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Calculate RMSLE\ny_test_log = np.log(y_test + 1)\ny_pred_log = np.log(y_pred + 1)\nrmsle = np.sqrt(mean_squared_error(y_test_log, y_pred_log))\n\n# Print the evaluation metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"RMSLE: {rmsle:.4f}\")\n"], "term_out": ["Accuracy: 0.7485\nF1 Score: 0.7532\nExecution time: 2 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 17, in <module>\n    X_train, X_test, y_train, y_test = train_test_test_split(\nNameError: name 'train_test_test_split' is not defined. Did you mean: 'train_test_split'?\nExecution time: a second seconds (time limit is an hour).", "Accuracy: 0.7380\nF1 Score: 0.7484\nRMSLE: 0.3548\nExecution time: 8 seconds seconds (time limit is an hour).", "Accuracy: 0.7485\nF1 Score: 0.7586\nExecution time: 4 minutes seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 17, in <module>\n    X_train, X_test, y_train, y_test = train_test_test_split(\nNameError: name 'train_test_test_split' is not defined. Did you mean: 'train_test_split'?\nExecution time: a second seconds (time limit is an hour).", "Cross-validated Accuracy: 0.7387\nCross-validated F1 Score: 0.7480\nCross-validated RMSLE: -0.3543\nExecution time: a minute seconds (time limit is an hour).", "Accuracy: 0.7536\nF1 Score: 0.7628\nRMSLE: 0.3441\nExecution time: 6 seconds seconds (time limit is an hour)."], "analysis": ["The logistic regression model achieved an accuracy of 74.85% and an F1 score of 75.32% on the test set. These results indicate a relatively good performance in predicting diabetes status in a balanced dataset. The model's ability to distinguish between the two classes (no diabetes vs. prediabetes or diabetes) is satisfactory, given the F1 score.", "The script contains a bug due to a typo in the function name `train_test_test_split` which should be corrected to `train_test_split`. This typo has caused a NameError, preventing the script from executing further and evaluating the model.", "The model achieved an accuracy of 73.80%, an F1 score of 74.84%, and an RMSLE of 0.3548. These metrics indicate that the Random Forest Classifier performed reasonably well in predicting diabetes status from the balanced dataset. The F1 score suggests a good balance between precision and recall, while the RMSLE indicates the model's error in terms of log differences is moderate.", "The SVM model trained on the balanced diabetes dataset achieved an accuracy of 74.85% and an F1 score of 75.86%. These metrics indicate a relatively good performance in distinguishing between individuals with and without diabetes, considering the model's simplicity and the challenging nature of medical prediction tasks.", "There is a typo in the function name used for splitting the dataset. The correct function name should be `train_test_split`, but it is mistakenly written as `train_test_test_split`. To fix this bug, replace `train_test_test_split` with `train_test_split` in the code.", "The RandomForestClassifier model achieved a cross-validated accuracy of 73.87%, F1 score of 74.80%, and RMSLE of 0.3543 on the balanced diabetes dataset. These results suggest that the model is fairly effective at predicting diabetes status using the provided health indicators, with a moderate level of accuracy and predictive performance.", "The model achieved an accuracy of 75.36%, an F1 score of 76.28%, and an RMSLE of 0.3441. These results suggest that the Gradient Boosting Classifier, along with proper preprocessing, can effectively predict diabetes status from the BRFSS 2015 dataset with a balanced binary target. The RMSLE indicates a moderate error in the logarithmic scale of predictions."], "exp_name": "0-foamy-sparkling-jackal", "metrics": [0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
