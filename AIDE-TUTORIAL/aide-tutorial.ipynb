{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a hacer un pequeño tutorial sobre como usar AIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver una forma muy fácil y visual de como usar AIDE con los LLM de OpenAI, para ello necesitamos tener crédito en nuestra cartera de la API de OpenAI --> <a href=\"https://platform.openai.com/settings/organization/billing/overview\">Para ver como meter saldo visitar esta página</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "from openai import OpenAI\n",
    "import aide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las librerías que uso en este fichero las podemos encontrar en el fichero <b>aideml-requirements.txt</b>\n",
    "\n",
    "Hacemos uso del import de OpenAI para poder ver la cantidad de modelos a los que podemos acceder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_modelos = OpenAI().models.list().dict() # Con esto sacamos la lista de modelos y la convertimos en un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dall-e-3\n",
      "gpt-4-1106-preview\n",
      "whisper-1\n",
      "gpt-4o-2024-05-13\n",
      "babbage-002\n",
      "dall-e-2\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1-hd-1106\n",
      "tts-1-hd\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-3.5-turbo-instruct\n",
      "text-embedding-3-small\n",
      "gpt-4-turbo-2024-04-09\n",
      "tts-1\n",
      "gpt-4-turbo\n",
      "text-embedding-3-large\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4-0125-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-4-turbo-preview\n",
      "tts-1-1106\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-4\n",
      "text-embedding-ada-002\n",
      "gpt-4-1106-vision-preview\n",
      "davinci-002\n",
      "gpt-4-0613\n",
      "gpt-4-vision-preview\n",
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "for modelo in lista_modelos[\"data\"]:\n",
    "    print(modelo[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la salida que deberías obtener al tener un mínimo de saldo en la cuenta, hay que pensar que cada modelo tiene su propio precio por token y que podemos consultar en la propia página de la <a href=\"https://openai.com/api/pricing/\">API de OpenAI</a>. Hay que tener en cuenta que los precios de entrada y salida son para cada millos de tokens procesados.\n",
    "\n",
    "Una vez elegido el modelo que queremos usar tenemos dos opciones de seleccionarlo,\n",
    "- (1) Cambiarlo por consola con el comando: <b>agent.code.model=\"modelo-a-usar\"</b>\n",
    "\n",
    "- (2) Cambiarlo en el fichero <b>config.yaml</b> de la extensión aide.\n",
    "\n",
    "Yo uso siempre la segunda forma, para ello nos dirijimos a la carpeta de nuetro entorno virtual --> navegamos hasta la carpeta <b>lib</b> --> seleccionamos la versión, en mi caso la <b>3.10.0</b> --> buscamos la carpeta <b>aide</b> --> dentro de la carpeta <b>utils</b> encontraremos el fichero <b>config.yaml</b>.\n",
    "\n",
    "En este fichero lo único que tenemos que cambiar son los dos parámetros llamados \"model\" tanto en la generación como en la validación, por defecto están asignados en \"gpt-4-turbo\" que es un buen modelo que no consume tanto crédito como \"gpt-4o\" o \"gpt-4\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a seleccionar algún problema medianamente complejo de Kaggle y pasárselo al AIDE.\n",
    "\n",
    "Como primer dataset voy a usar un csv con muestras de agua para ver si consigue hacer un clasificador de estas sobre su potabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las variables que vamos a utilizar\n",
    "data_dir = \"./Data/\"\n",
    "goal = \"\"\"\n",
    "The dataset contains informatino about various factors affecting water quality. It includes measurements of pH, hardness, solids ...\n",
    "\n",
    "Make an analysis of the dataset and a classifier not using a RandomForest\n",
    "\"\"\"\n",
    "eval=\"Use RMSLE, f1-score and accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estas declaraciones podemos ya probar el AIDE, para declaralo simplemente llamamos al constructor y le pasamos los 3 parámetros que acabamos de definir.\n",
    "\n",
    "El tercer parámetro <b>goal</b> es opcional pero siempre está bien poner como queremos que el modelo actue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp = aide.Experiment(\n",
    "    data_dir=data_dir,\n",
    "    goal=goal,\n",
    "    eval=eval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada la instancia solo tenemos que hacerla correr, hay que tener en cuenta que el número de pasos que pongamos incrementará las iteraciones que el modelo intenta mejorar su solución propuesta y por tanto aumentará la cuota de la consulta.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/aide/utils/tree_export.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  layout = (layout - layout.min(axis=0)) / (layout.max(axis=0) - layout.min(axis=0))\n"
     ]
    }
   ],
   "source": [
    "best_solution = exp.run(steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ejecutado vamos a sacar por pantalla cuál es la mejor <b>matrix validation</b> que nos indica cuan buena es la solución propuesta así como el código generado --> este se guarda en un fichero dentro del directorio por si no queremos mostrar por pantalla el código.\n",
    "\n",
    "Además del código, AIDE genera un documento .html con un árbol para que podamos ver como ha desarrollado la o las soluciones propuestas así como por que ha elegido esa (interpretable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor solución tiene una validación de --> 0.32162281958137\n",
      "El código de la mejor propuesta es: \n",
      " import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import mean_squared_log_error, f1_score, accuracy_score\n",
      "\n",
      "# Load the data\n",
      "data = pd.read_csv(\"./input/water_potability.csv\")\n",
      "\n",
      "# Impute missing values\n",
      "imputer = SimpleImputer(strategy=\"median\")\n",
      "data_imputed = imputer.fit_transform(data)\n",
      "data = pd.DataFrame(data_imputed, columns=data.columns)\n",
      "\n",
      "# Feature and target separation\n",
      "X = data.drop(\"Potability\", axis=1)\n",
      "y = data[\"Potability\"]\n",
      "\n",
      "# Split the data into training and validation sets\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Scale the features\n",
      "scaler = StandardScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train)\n",
      "X_val_scaled = scaler.transform(X_val)\n",
      "\n",
      "# Initialize and train the SVM model\n",
      "svm_model = SVC(probability=True)\n",
      "svm_model.fit(X_train_scaled, y_train)\n",
      "\n",
      "# Predict on the validation set\n",
      "y_pred = svm_model.predict(X_val_scaled)\n",
      "y_pred_proba = svm_model.predict_proba(X_val_scaled)[:, 1]\n",
      "\n",
      "# Calculate RMSLE, f1-score, and accuracy\n",
      "rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred_proba))\n",
      "f1 = f1_score(y_val, y_pred)\n",
      "accuracy = accuracy_score(y_val, y_pred)\n",
      "\n",
      "# Print the evaluation metrics\n",
      "print(f\"RMSLE: {rmsle}\")\n",
      "print(f\"F1-Score: {f1}\")\n",
      "print(f\"Accuracy: {accuracy}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"La mejor solución tiene una validación de --> {best_solution.valid_metric}\")\n",
    "print(f\"El código de la mejor propuesta es: \\n {best_solution.code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esta ejecución ha costado <label style=\"color:red\">$0.46</label>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución del código proporcionado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE: 0.32163927312616114\n",
      "F1-Score: 0.43454038997214484\n",
      "Accuracy: 0.6905487804878049\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_squared_log_error, f1_score, accuracy_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"./Data/water_potability.csv\")\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "data_imputed = imputer.fit_transform(data)\n",
    "data = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "\n",
    "# Feature and target separation\n",
    "X = data.drop(\"Potability\", axis=1)\n",
    "y = data[\"Potability\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Initialize and train the SVM model\n",
    "svm_model = SVC(probability=True)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = svm_model.predict(X_val_scaled)\n",
    "y_pred_proba = svm_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Calculate RMSLE, f1-score, and accuracy\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred_proba))\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"RMSLE: {rmsle}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este dataset no ha resultado muy bueno, vamos a probar con otros datasets, como por ejemplo el del titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear las variables que vamos a utilizar para el aide:\n",
    "data_dir = \"./Data-Titanic\"\n",
    "goal = \"You have to predict if a passenger survived the sinking of the Titanic or not. For each in the test set, you must predict a 0 or 1 value for the variable. In Data folder you have the train and test data and an example of how the submission.csv file should be, store it inside ./Data/ as submission.csv. Train.csv contains the details of a subset of the passengers on board and importantly, sill reveal whether thry survived or not. The Test.csv does not disclose the ground truth for each passenger.\"\n",
    "eval = \"The metric to use ir the percentatge of passengers you correctly predict. This is known asa accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp = aide.Experiment(\n",
    "    data_dir=data_dir,\n",
    "    goal=goal,\n",
    "    eval=eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan + code extraction failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro/Documents/Repositorios/alejandromadrid/.aide-venv/lib/python3.10/site-packages/aide/utils/tree_export.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  layout = (layout - layout.min(axis=0)) / (layout.max(axis=0) - layout.min(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan + code extraction failed, retrying...\n"
     ]
    }
   ],
   "source": [
    "best_solution = exp.run(steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mejor solución tiene una validación de --> 0.8379888268156425\n"
     ]
    }
   ],
   "source": [
    "print(f\"La mejor solución tiene una validación de --> {best_solution.valid_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El código de la mejor propuesta es: \n",
      " import pandas as pd\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Load data\n",
      "train_data = pd.read_csv(\"./input/train.csv\")\n",
      "test_data = pd.read_csv(\"./input/test.csv\")\n",
      "\n",
      "# Preprocessing\n",
      "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
      "X = train_data[features]\n",
      "y = train_data[\"Survived\"]\n",
      "X_test = test_data[features]\n",
      "\n",
      "# Preprocessing for numerical data\n",
      "numerical_transformer = SimpleImputer(strategy=\"median\")\n",
      "\n",
      "# Preprocessing for categorical data\n",
      "categorical_transformer = Pipeline(\n",
      "    steps=[\n",
      "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
      "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
      "    ]\n",
      ")\n",
      "\n",
      "# Bundle preprocessing for numerical and categorical data\n",
      "preprocessor = ColumnTransformer(\n",
      "    transformers=[\n",
      "        (\"num\", numerical_transformer, [\"Age\", \"Fare\"]),\n",
      "        (\n",
      "            \"cat\",\n",
      "            categorical_transformer,\n",
      "            [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"],\n",
      "        ),\n",
      "    ]\n",
      ")\n",
      "\n",
      "# Define the model\n",
      "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
      "\n",
      "# Create and evaluate the pipeline\n",
      "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
      "\n",
      "# Split data into train and validation subsets\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(\n",
      "    X, y, train_size=0.8, test_size=0.2, random_state=0\n",
      ")\n",
      "\n",
      "# Preprocessing of training data, fit model\n",
      "pipeline.fit(X_train, y_train)\n",
      "\n",
      "# Preprocessing of validation data, get predictions\n",
      "preds = pipeline.predict(X_valid)\n",
      "\n",
      "# Evaluate the model\n",
      "score = accuracy_score(y_valid, preds)\n",
      "print(\"Validation accuracy:\", score)\n",
      "\n",
      "# Preprocessing of test data, fit model\n",
      "preds_test = pipeline.predict(X_test)\n",
      "\n",
      "# Save test predictions to file\n",
      "output = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": preds_test})\n",
      "output.to_csv(\"./working/submission.csv\", index=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"El código de la mejor propuesta es: \\n {best_solution.code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esta ejecución ha costado <label style=\"color:red\">$0.63</label>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vamos a probar la solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8379888268156425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"./Data-Titanic/train.csv\")\n",
    "test_data = pd.read_csv(\"./Data-Titanic/test.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "X = train_data[features]\n",
    "y = train_data[\"Survived\"]\n",
    "X_test = test_data[features]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, [\"Age\", \"Fare\"]),\n",
    "        (\n",
    "            \"cat\",\n",
    "            categorical_transformer,\n",
    "            [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Create and evaluate the pipeline\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "\n",
    "# Split data into train and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, train_size=0.8, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Preprocessing of training data, fit model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = accuracy_score(y_valid, preds)\n",
    "print(\"Validation accuracy:\", score)\n",
    "\n",
    "# Preprocessing of test data, fit model\n",
    "preds_test = pipeline.predict(X_test)\n",
    "\n",
    "# Save test predictions to file\n",
    "output = pd.DataFrame({\"PassengerId\": test_data.PassengerId, \"Survived\": preds_test})\n",
    "output.to_csv(\"./Data-Titanic/submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, en este algoritmo se ha portado mejor produciendo una predicción correcta de > 83% \n",
    "\n",
    "AIDE es una herramienta muy potente para lo fácil y poco que cuesta trabajar con ella. Supongo que para proyectos de mayor envergaadura será más costoso pero aún así para tener una 'baseline' o una base desde donde partir a trabajar es una herramiento muy poderosa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aide-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
